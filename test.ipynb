{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f256d601-222a-479c-8ec8-3a5122429dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32adff45-7fa3-4354-b6be-d6f02d1e33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3f3d044-bdad-4f4b-b1c7-1692779ec25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e7b10c-159e-4c1e-a681-ecf70b5493a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/opt-2.7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f02d9da-fe2e-4049-a50e-9589df5a4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/gpt-j-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc75c6b-15b3-4ce1-a026-e29c4e38f0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af650f0-f4a0-4dd0-9a32-7d70194c9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_relational import Prompt\n",
    "\n",
    "prompts = [\n",
    "    Prompt(\n",
    "        \"Paris is located in the country of\",\n",
    "        answer=\"France\",\n",
    "        subject=\"Paris\",\n",
    "    ),\n",
    "    Prompt(\n",
    "        \"Shanghai is located in the country of\",\n",
    "        answer=\"China\",\n",
    "        subject=\"Shanghai\",\n",
    "    ),\n",
    "    Prompt(\n",
    "        \"Kyoto is located in the country of\",\n",
    "        answer=\"Japan\",\n",
    "        subject=\"Kyoto\",\n",
    "    ),\n",
    "    Prompt(\n",
    "        \"San Jose is located in the country of\",\n",
    "        answer=\"Costa Rica\",\n",
    "        subject=\"San Jose\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d15b2e7-d52b-4ea1-b32d-fe4e1de6f936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
      "      (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd215343-99a6-43b3-9ba8-9851107b0b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating 4 prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [01:40<00:00, 100.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating 4 prompts / 4\n"
     ]
    }
   ],
   "source": [
    "from linear_relational import Trainer\n",
    "trainer = Trainer(model, tokenizer)\n",
    "\n",
    "lre = trainer.train_lre(\n",
    "    relation=\"located in country\",\n",
    "    subject_layer=8,\n",
    "    object_layer=11,\n",
    "    prompts=prompts,\n",
    ")\n",
    "\n",
    "# for prompt in prompts:\n",
    "#     input_ids = tokenizer(prompt.text, return_tensors=\"pt\")\n",
    "#     logits = model(**input_ids)[\"logits\"][0][-1]\n",
    "#     print(f\"{text} {tokenizer.decode(logits.argmax())} (Answer: {prompt.answer})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9743ec25-ecaf-4966-9e13-1b8a7155cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lre(located in country, layers 8 -> 11, mean)\n"
     ]
    }
   ],
   "source": [
    "print(lre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f675f8-543f-404f-a0ec-bf43668a69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Paris is located in the country of\"\n",
    "sbj_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a15411dd-c3f5-4de7-a817-2c6b7c03a3e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_text: Paris is located in the country of; subject position: 0\n",
      "From logits:  France\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m object_acts_estimate \u001b[38;5;241m=\u001b[39m lre(subject_acts)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m27\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     object_acts_estimate \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mlayers[i](object_acts_estimate\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     14\u001b[0m object_acts_estimate \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(object_acts_estimate)\n\u001b[1;32m     15\u001b[0m obj_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(object_acts_estimate\u001b[38;5;241m.\u001b[39margmax())\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.02-1/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.02-1/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.02-1/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:524\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    521\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    525\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    526\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    527\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    528\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    529\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    530\u001b[0m )\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    532\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.02-1/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.02-1/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2024.02-1/lib/python3.11/site-packages/transformers/models/opt/modeling_opt.py:151\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# if key_value_states are provided this layer is used as a cross-attention layer\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# for the decoder\u001b[39;00m\n\u001b[1;32m    149\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[1;32m    154\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# test LRE\n",
    "print(f\"sample_text: {sample_text}; subject position: {sbj_pos}\")\n",
    "input_ids = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**input_ids, output_hidden_states=True)\n",
    "logits = outputs[\"logits\"][0][-1]\n",
    "print(\"From logits:\", tokenizer.decode(logits.argmax()))\n",
    "\n",
    "subject_acts = outputs.hidden_states[8][0][sbj_pos, :]  # 22-th layer, 0-th batch\n",
    "object_acts_estimate = lre(subject_acts)\n",
    "\n",
    "for i in range(12, 27):\n",
    "    object_acts_estimate = model.model.decoder.layers[i](object_acts_estimate.unsqueeze(0))\n",
    "object_acts_estimate = model.lm_head(object_acts_estimate)\n",
    "obj_token = tokenizer.decode(object_acts_estimate.argmax())\n",
    "      \n",
    "print(f\"From LRE   : {obj_token}\")\n",
    "top5 = lm_head.topk(5).indices.tolist()\n",
    "print(\"top 5      :\", [tokenizer.decode(i) for i in top5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a343e98-cd56-4ce2-a325-ac1f8118680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text = \"Stephen might be hiring Marcus.\"\n",
    "emb = model.encode(text)\n",
    "print(emb.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
